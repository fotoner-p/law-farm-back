# -*- coding: utf-8 -*-
"""project-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KukBVu5LsyJDHXLENqOZMCtTcv5ZLiX_

# 필수 라이브러리설치
"""

!pip
install
git
!git
clone
https: // github.com / SOMJANG / Mecab - ko -
for -Google - Colab.git
    !bash
    Mecab - ko -
    for -Google - Colab / install_mecab - ko_on_colab190912.sh

!apt - qq - y
install
fonts - nanum

"""# 데이터 전처리"""

from konlpy.tag import Mecab

import os

os.chdir('/content/drive/My Drive/project/')

import pandas as pd
import re

df = pd.read_json('all_paragraphs.json')
texts = df['text'].tolist()
nlp = Mecab()

df


def preprocess(content: list):
    re_content = []
    for item in content:
        re_str = item
        result = [item[0] + "/" + item[1] for item in nlp.pos(re_str)]
        result_str = " ".join(result)
        re_content.append(result_str)

    return re_content


texts = preprocess(texts)

from pprint import pprint

pprint(texts[:10])

from wordcloud import WordCloud
import matplotlib.pyplot as plt

fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
re_form = "[^ㄱ-ㅎㅏ-ㅣ가-힣0-9 ]"


def token_getter(content: list):
    re_content = []
    for item in content:
        re_str = re.sub(re_form, "", item)

        result = [item[0] for item in nlp.pos(re_str)]
        result_str = " ".join(result)
        re_content.append(result_str)

    return re_content


content = token_getter(df['text'].tolist())

wc = WordCloud(
    font_path=fontpath,
    width=800,
    height=400,
    background_color="white",
)
wc = wc.generate_from_text(" ".join(content))

plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

content

fst_content = token_getter(df[df["type"] == 0]['text'].tolist())
snd_content = token_getter(df[df["type"] == 1]['text'].tolist())
trd_content = token_getter(df[df["type"] == 2]['text'].tolist())

wc = WordCloud(
    font_path=fontpath,
    width=800,
    height=400,
    background_color="white",
)
wc = wc.generate_from_text(" ".join(fst_content))

plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

wc = WordCloud(
    font_path=fontpath,
    width=800,
    height=400,
    background_color="white",
)
wc = wc.generate_from_text(" ".join(snd_content))

plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

wc = WordCloud(
    font_path=fontpath,
    width=800,
    height=400,
    background_color="white",
)
wc = wc.generate_from_text(" ".join(trd_content))

plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import json

vocab_size = 30000
tokenizer = Tokenizer(vocab_size)
tokenizer.fit_on_texts(texts)

tokenizer_json = tokenizer.to_json()
with open('tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

import seaborn as sns
import matplotlib.pyplot as plt

texts_sequences = tokenizer.texts_to_sequences(texts)

content_length = []
for arr in texts_sequences:
    content_length.append(len(arr))

# 컨텐츠 길이
sns.distplot(content_length)
plt.show()

# content_max_len = max(content_length)
# content_max_len = 300 # 길이기 2500이 넘어가는 이상치 존재, 평균적으로는 그보다 한참 밑
# title_max_len = max(title_length)
# print(title_max_len)

max(content_length)

sum(content_length) / len(content_length)

plt.boxplot(content_length)
plt.show()

texts = [text.split(' ') for text in texts]

from gensim.models.word2vec import Word2Vec

w2v_model = Word2Vec(sentences=texts, size=100, window=4, min_count=5, workers=4, iter=80)

# w2v_model.save("./build/proto")
w2v_model.wv.save_word2vec_format("./build/proto")



!python3 - m
gensim.scripts.word2vec2tensor - -input. / build / proto - -output. / build / proto


def vectors(document_list):
    document_embedding_list = []

    # 각 문서에 대해서
    for line in document_list:
        doc2vec = None
        count = 0
        for word in line:
            if word in w2v_model.wv.vocab:
                count += 1
                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.
                if doc2vec is None:
                    doc2vec = w2v_model[word]
                else:
                    doc2vec = doc2vec + w2v_model[word]

        if doc2vec is not None:
            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.
            doc2vec = doc2vec / count
            document_embedding_list.append(doc2vec)

    # 각 문서에 대한 문서 벡터 리스트를 리턴
    return document_embedding_list


document_embedding_list = vectors(texts)
print('문서 벡터의 수 :', len(document_embedding_list))

import gensim

paragraph_model = gensim.models.KeyedVectors(vector_size=100)
paragraph_model.add(weights=document_embedding_list, entities=df['fullname'].tolist())

paragraph_model.vocab

target_key = '자동차손해배상 보장법 제45조의2 제1항'

paragraph_model.similar_by_word(target_key)

import json

with open('all_paragraphs.json', 'r') as f:
    paragraph_data: dict = json.load(f)

paragraph_data[:3]

paragraph_data = {
    paragraph['fullname']: {
        'statute': paragraph['statute'],
        'article': paragraph['article'],
        'paragraph': paragraph['paragraph'],
        'text': paragraph['text'],
    } for paragraph in paragraph_data
}

target_key = '자동차손해배상 보장법 제45조의2 제1항'

print(f'{target_key}: {paragraph_data[target_key]["text"]}\n')

for key, weight in paragraph_model.similar_by_word(target_key, 3):
    print(f'{key}: {paragraph_data[key]["text"]}')

with open('all_statutes.json', 'r') as f:
    statute_data: dict = json.load(f)

article_list = []
article_weights = []
article_text = dict()
article_type = dict()
for statute in statute_data:
    if 'articles' not in statute.keys():
        continue

    for article in statute['articles']:
        doc2vec = None

        article_text[article['fullname']] = article['text']
        article_type[article['fullname']] = article['type']

        count = 0
        if 'paragraphs' not in article.keys():
            words = preprocess([article['text']])[0].split(' ')

            for word in words:
                if word in w2v_model.wv.vocab:
                    count += 1
                    # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.
                    if doc2vec is None:
                        doc2vec = w2v_model[word]
                    else:
                        doc2vec = doc2vec + w2v_model[word]

        else:
            for paragraph in article['paragraphs']:
                paragraph_key = paragraph['fullname']
                if paragraph_key in paragraph_model.vocab:
                    count += 1
                    # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.
                    if doc2vec is None:
                        doc2vec = paragraph_model[paragraph_key]
                    else:
                        doc2vec = doc2vec + paragraph_model[paragraph_key]

        if doc2vec is not None:
            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.
            doc2vec = doc2vec / count
            article_list.append(article['fullname'])
            article_weights.append(doc2vec)

article_model = gensim.models.KeyedVectors(vector_size=100)
article_model.add(weights=article_weights, entities=article_list)

target_key = '도로교통법 제15조의2'
# target_key = '주택건설기준 등에 관한 규정 제60조의7'
print(f'[target] {target_key}: {article_text[target_key]}\n')

for key, weight in article_model.similar_by_word(target_key):
    print(f'{key}: {article_text[key]}\n')

target_key = '도로교통법 제15조의2'
# target_key = '주택건설기준 등에 관한 규정 제60조의7'
print(f'[target] {target_key}: {article_text[target_key]}\n')

for key, weight in paragraph_model.similar_by_vector(article_model[target_key]):
    print(f'{key}: {paragraph_data[key]["text"]}\n')

article_model.wv.save_word2vec_format("./build/article_model")

import numpy as np

np.savetxt('article_tensor.tsv', article_weights, delimiter='\t')
with open('article_meta.tsv', 'w') as f:
    f.write(f'key\ttype[key]\n')
    for idx, key in enumerate(article_list):
        f.write(f'{key}\t{article_type[key]}\n')

!python3 - m
gensim.scripts.word2vec2tensor - -input. / build / article_model - -output. / build / article_model